{
 "cells": [
  {
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "from pyknp import Juman\n",
    "import os\n",
    "import json\n",
    "import numpy as np"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ●Bert設定"
   ]
  },
  {
   "source": [
    "# mask語予測と隣接文予測をするためのBERTモデル\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# do_lower_case=False, do_basic_tokenize=False\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "# model.eval()"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ●マスク語予測\n",
    "### masked_lm_labels: \n",
    "* masked language modeling labels: torch.LongTensor of shape [batch_size, sequence_length] with indices selected in [-1, 0, ..., vocab_size]. \n",
    "* All labels set to -1 are ignored (masked), the loss is only computed for the labels set in [0, ..., vocab_size]"
   ]
  },
  {
   "source": [
    "def predict_mask(text, tag_bert_tokens, mask_id):\n",
    "    true_tokens = []\n",
    "    for i in mask_id:\n",
    "        true_tokens.append(tag_bert_tokens[i]) \n",
    "        tag_bert_tokens[i] = \"[MASK]\"\n",
    "    print(\"# tokens: \\n{}\\n\".format(tag_bert_tokens))\n",
    "\n",
    "    ids = tokenizer.convert_tokens_to_ids(tag_bert_tokens)\n",
    "\n",
    "    # ベクトルを作成する\n",
    "    tokens_tensor = torch.tensor(ids).reshape(1, -1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor)\n",
    "        predictions = outputs[0]\n",
    "        print(predictions[mask_id])\n",
    "\n",
    "    # 上位10トークンの確率\n",
    "    print(\"# true: \\n{}\\n\".format(true_tokens))\n",
    "    print(\"# predicted: \\n\")\n",
    "    for i in mask_id:\n",
    "        scores, predicted_indexes = torch.topk(predictions[mask_id], k=5)\n",
    "        predicted_tokens = tokenizer.convert_ids_to_tokens(predicted_indexes[0].tolist())\n",
    "        print(\"# predicted_tokens: \\n{}\\n\".format(predicted_tokens))"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 3
  },
  {
   "source": [
    "text1 = \"\"\"\n",
    "the man went to the store.\"\"\"\n",
    "text2 = \"\"\"\n",
    "あなたは？\n",
    "\"\"\"\n",
    "\n",
    "# 文書数\n",
    "cnt = 1"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 4
  },
  {
   "source": [
    "if cnt == 1:\n",
    "    text2 = \"\"\n",
    "    bert_tokens = tokenizer.tokenize(text1)\n",
    "\n",
    "    tag_bert_tokens = [\"[CLS]\"] + bert_tokens[:126] + [\"[SEP]\"]\n",
    "    print(\"tokens: \\n{}\\n\".format(tag_bert_tokens))\n",
    "\n",
    "    print([[i, j] for i, j in zip(range(len(tag_bert_tokens)), tag_bert_tokens)])\n",
    "\n",
    "elif cnt == 2:\n",
    "    tokens_a = Mecab_Tokenizer.tokenize(text1)\n",
    "    tokens_b = Mecab_Tokenizer.tokenize(text2)\n",
    "    tokens_a = tokenizer.tokenize(\" \".join(tokens_a))\n",
    "    tokens_b = tokenizer.tokenize(\" \".join(tokens_b))\n",
    "    tokens_a = ['[CLS]'] + tokens_a + ['[SEP]']\n",
    "    tokens_b = tokens_b + ['[SEP]']\n",
    "    tag_bert_tokens = tokens_a + tokens_b\n",
    "    \n",
    "    print(\"tokens: \\n{}\\n\".format(tag_bert_tokens))\n",
    "\n",
    "    print([[i, j] for i, j in zip(range(len(tag_bert_tokens)), tag_bert_tokens)])"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tokens: \n['[CLS]', 'the', 'man', 'went', 'to', 'the', 'store', '.', '[SEP]']\n\n[[0, '[CLS]'], [1, 'the'], [2, 'man'], [3, 'went'], [4, 'to'], [5, 'the'], [6, 'store'], [7, '.'], [8, '[SEP]']]\n"
    }
   ],
   "metadata": {},
   "execution_count": 5
  },
  {
   "source": [
    "mask_id = [6]\n",
    "predict_mask(text1 + text2, tag_bert_tokens, mask_id)"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "# tokens: \n['[CLS]', 'the', 'man', 'went', 'to', 'the', '[MASK]', '.', '[SEP]']\n\ntensor([[-3.8511, -3.7515, -3.8620,  ..., -3.4319, -3.8656, -3.8376]])\n# true: \n['store']\n\n# predicted: \n\n# predicted_tokens: \n['door', 'window', 'bathroom', 'kitchen', 'bed']\n\n"
    }
   ],
   "metadata": {},
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ●隣接文予測\n",
    "### next_sentence_label: \n",
    "* next sentence classification loss: torch.LongTensor of shape [batch_size] with indices selected in [0, 1]. \n",
    "* 0 => next sentence is the continuation, \n",
    "* 1 => next sentence is a random sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"\"\"\n",
    "包丁とは、調理に使う刃物のことである。\n",
    "\"\"\"\n",
    "text2 = \"\"\"\n",
    "出刃包丁、刺身包丁、菜切り包丁など、多くの種類がある。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text1: \n",
      "包丁とは、調理に使う刃物のことである。\n",
      "\n",
      "text2: \n",
      "出刃包丁、刺身包丁、菜切り包丁など、多くの種類がある。\n",
      "\n",
      "tokens: \n",
      "['[CLS]', '包丁', 'と', 'は', '、', '調理', 'に', '使う', '刃物', 'の', 'こと', 'で', 'ある', '。', '[SEP]', '[UNK]', '、', '[UNK]', '、', '[UNK]', 'など', '、', '多く', 'の', '種類', 'が', 'ある', '。', '[SEP]']\n",
      "\n",
      "next sentence: 20.38%\n",
      "random sentence: 79.62%\n",
      "\n",
      "second-text is random next: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Mecab_Tokenizer = MecabTokenizer()\n",
    "tokens1 = Mecab_Tokenizer.tokenize(text1)\n",
    "tokens2 = Mecab_Tokenizer.tokenize(text2)\n",
    "# print(\"juman++: {} {}\".format(tokens1, tokens2))\n",
    "\n",
    "bert_tokens1 = tokenizer.tokenize(\" \".join(tokens1))\n",
    "bert_tokens2 = tokenizer.tokenize(\" \".join(tokens2))\n",
    "# print(\"BertTokenizer: {} {}\".format(bert_tokens1, bert_tokens2))\n",
    "\n",
    "tag_bert_tokens = (\n",
    "    [\"[CLS]\"] + bert_tokens1[:126] + [\"[SEP]\"] + bert_tokens2[:126] + [\"[SEP]\"]\n",
    ")\n",
    "\n",
    "print(\"text1: {}\\ntext2: {}\".format(text1, text2))\n",
    "print(\"tokens: \\n{}\\n\".format(tag_bert_tokens))\n",
    "# print(\"len: {}\\n\".format(len(tag_bert_tokens)))\n",
    "\n",
    "ids = tokenizer.convert_tokens_to_ids(tag_bert_tokens)\n",
    "# print(\"idを振る: {}\\n\".format(ids))\n",
    "\n",
    "# ベクトルを作成する\n",
    "tokens_tensor = torch.tensor(ids).reshape(1, -1)\n",
    "# print(tokens_tensor)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output, label = model(tokens_tensor)\n",
    "    \n",
    "m = nn.Softmax(dim=-1)\n",
    "pp = [\"{:.2f}%\".format(i*100) for i in m(label[0])]\n",
    "\n",
    "print(\"next sentence: {}\\nrandom sentence: {}\\n\".format(pp[0], pp[1]))\n",
    "print(\"second-text is random next: {}\\n\".format(bool(np.argmax(pp))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}