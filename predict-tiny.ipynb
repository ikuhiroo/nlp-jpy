{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM, BertForPreTraining, modeling, BertConfig, BertForNextSentencePrediction \n",
    "import os\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1st version to reproduce our results in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"model_dir\"] = \"/Users/ikuhiro/Desktop/BERT/model/General_TinyBERT_4L_312D/\"\n",
    "os.environ[\"pytorch_model\"] = \"/Users/ikuhiro/Desktop/BERT/model/General_TinyBERT_4L_312D/pytorch_model.bin\"\n",
    "os.environ[\"vocab_txt\"] = \"/Users/ikuhiro/Desktop/BERT/model/General_TinyBERT_4L_312D/vocab.txt\"\n",
    "os.environ[\"bert_config\"] = \"/Users/ikuhiro/Desktop/BERT/model/General_TinyBERT_4L_312D/config.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"model_dir\"] = \"/Users/ikuhiro/Desktop/BERT/model/General_TinyBERT_6L_768D/\"\n",
    "os.environ[\"pytorch_model\"] = \"/Users/ikuhiro/Desktop/BERT/model/General_TinyBERT_6L_768D/pytorch_model.bin\"\n",
    "os.environ[\"vocab_txt\"] = \"/Users/ikuhiro/Desktop/BERT/model/General_TinyBERT_6L_768D/vocab.txt\"\n",
    "os.environ[\"bert_config\"] = \"/Users/ikuhiro/Desktop/BERT/model/General_TinyBERT_6L_768D/bert_config.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2nd version (2019/11/18) trained with more (book+wiki) and no [MASK] corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"model_dir\"] = \"/Users/ikuhiro/Desktop/BERT/model/2nd_General_TinyBERT_4L_312D/\"\n",
    "os.environ[\"pytorch_model\"] = \"/Users/ikuhiro/Desktop/BERT/model/2nd_General_TinyBERT_4L_312D/pytorch_model.bin\"\n",
    "os.environ[\"vocab_txt\"] = \"/Users/ikuhiro/Desktop/BERT/model/2nd_General_TinyBERT_4L_312D/vocab.txt\"\n",
    "os.environ[\"bert_config\"] = \"/Users/ikuhiro/Desktop/BERT/model/2nd_General_TinyBERT_4L_312D/bert_config.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"model_dir\"] = \"/Users/ikuhiro/Desktop/BERT/model/2nd_General_TinyBERT_6L_768D/\"\n",
    "os.environ[\"pytorch_model\"] = \"/Users/ikuhiro/Desktop/BERT/model/2nd_General_TinyBERT_6L_768D/pytorch_model.bin\"\n",
    "os.environ[\"vocab_txt\"] = \"/Users/ikuhiro/Desktop/BERT/model/2nd_General_TinyBERT_6L_768D/vocab.txt\"\n",
    "os.environ[\"bert_config\"] = \"/Users/ikuhiro/Desktop/BERT/model/2nd_General_TinyBERT_6L_768D/bert_config.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ●Bert設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask語予測と隣接文予測をするためのBERTモデル\n",
    "model = BertForMaskedLM.from_pretrained(os.environ[\"model_dir\"])\n",
    "tokenizer = BertTokenizer.from_pretrained(os.environ[\"model_dir\"], do_lower_case=True, do_basic_tokenize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor(0.0017, grad_fn=<MeanBackward0>)\ntensor(0.0001, grad_fn=<MeanBackward0>)\ntensor(0.0001, grad_fn=<MeanBackward0>)\ntensor(1.9590e-05, grad_fn=<MeanBackward0>)\ntensor(0.0017, grad_fn=<MeanBackward0>)\n"
    }
   ],
   "source": [
    "print(model.bert.embeddings.word_embeddings.weight.mean())\n",
    "print(model.bert.encoder.layer[0].attention.output.dense.weight.mean())\n",
    "print(model.bert.pooler.dense.weight.mean())\n",
    "print(model.cls.predictions.transform.dense.weight.mean())\n",
    "print(model.cls.predictions.decoder.weight.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_list = list(tokenizer.vocab.keys())\n",
    "# tokenizer.vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2171 - name\n",
    "# tokenizer.convert_ids_to_tokens([2171])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## bert based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask語予測と隣接文予測をするためのBERTモデル\n",
    "# do_lower_case=False, do_basic_tokenize=False\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model.bert.embeddings.word_embeddings.weight.mean())\n",
    "# print(model.bert.encoder.layer[0].attention.output.dense.weight.mean())\n",
    "# print(model.bert.encoder.layer[11].attention.output.dense.weight.mean())\n",
    "# print(model.bert.pooler.dense.weight.mean())\n",
    "# print(model.cls.predictions.transform.dense.weight.mean())\n",
    "# print(model.cls.predictions.decoder.weight.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_list = list(tokenizer.vocab.keys())\n",
    "# tokenizer.vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2171 - name\n",
    "# tokenizer.convert_ids_to_tokens([2171])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ●マスク語予測\n",
    "### masked_lm_labels: \n",
    "* masked language modeling labels: torch.LongTensor of shape [batch_size, sequence_length] with indices selected in [-1, 0, ..., vocab_size]. \n",
    "* All labels set to -1 are ignored (masked), the loss is only computed for the labels set in [0, ..., vocab_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_mask(text, tag_bert_tokens, mask_id, segments_ids):\n",
    "    true_tokens = []\n",
    "    for i in mask_id:\n",
    "        true_tokens.append(tag_bert_tokens[i]) \n",
    "        tag_bert_tokens[i] = \"[MASK]\"\n",
    "    ids = tokenizer.convert_tokens_to_ids(tag_bert_tokens)\n",
    "    print(\"# ids: {}\\n\".format(ids))\n",
    "    print(\"# tokens: {}\\n\".format(tokenizer.convert_ids_to_tokens(ids)))\n",
    "\n",
    "    # ベクトルを作成する\n",
    "    input_ids = torch.tensor([ids])  # unsqueeze(0) / reshape(1, -1)\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    print(\"# input_ids: {}\\n\".format(input_ids))\n",
    "    print(\"# segments_tensors: {}\\n\".format(segments_tensors))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predictions = model(input_ids, segments_tensors)\n",
    "    \n",
    "    print(\"# true: \\n{}\\n\".format(true_tokens))\n",
    "    for i in mask_id:        \n",
    "        scores, predicted_indexes = torch.topk(predictions[0, i], k=5)\n",
    "        predicted_tokens = tokenizer.convert_ids_to_tokens(predicted_indexes.tolist())\n",
    "        print(\"# scores: {}\\n\".format(scores))\n",
    "        print(\"# predicted_indexes: {}\\n\".format(predicted_indexes))\n",
    "        print(\"# predicted_tokens: {}\\n\".format(predicted_tokens))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"\"\"\n",
    "the man went to the store . \n",
    "\"\"\"\n",
    "# 文書数\n",
    "cnt = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"\"\"\n",
    "he bought a gallon of milk .\n",
    "\"\"\"\n",
    "# 文書数\n",
    "cnt = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"\"\"\n",
    "the man went to the store .\n",
    "\"\"\"\n",
    "text2 = \"\"\"\n",
    "he bought a gallon of milk .\n",
    "\"\"\"\n",
    "# 文書数\n",
    "cnt = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"\"\"\n",
    "Who was Jim Henson ?\n",
    "\"\"\"\n",
    "text2 = \"\"\"\n",
    "Jim Henson was a puppeteer\n",
    "\"\"\"\n",
    "# 文書数\n",
    "cnt = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tokens: \n['[CLS]', 'the', 'man', 'went', 'to', 'the', 'store', '.', '[SEP]']\n\nsegments_ids: \n[0, 0, 0, 0, 0, 0, 0, 0, 0]\n\n[[0, '[CLS]'], [1, 'the'], [2, 'man'], [3, 'went'], [4, 'to'], [5, 'the'], [6, 'store'], [7, '.'], [8, '[SEP]']]\n"
    }
   ],
   "source": [
    "max_seq_len = 128\n",
    "if cnt == 1:\n",
    "    text2 = \"\"\n",
    "    bert_tokens = tokenizer.tokenize(text1)\n",
    "\n",
    "    tag_bert_tokens = [\"[CLS]\"] + bert_tokens[:max_seq_len] + [\"[SEP]\"]\n",
    "    segments_ids = [0]*len(tag_bert_tokens)\n",
    "    print(\"tokens: \\n{}\\n\".format(tag_bert_tokens))\n",
    "    print(\"segments_ids: \\n{}\\n\".format(segments_ids))\n",
    "\n",
    "    print([[i, j] for i, j in zip(range(len(tag_bert_tokens)), tag_bert_tokens)])\n",
    "\n",
    "elif cnt == 2:\n",
    "    tokens_a = tokenizer.tokenize(text1)\n",
    "    tokens_b = tokenizer.tokenize(text2)\n",
    "    tokens_a = ['[CLS]'] + tokens_a + ['[SEP]']\n",
    "    tokens_b = tokens_b + ['[SEP]']\n",
    "    tag_bert_tokens = tokens_a + tokens_b\n",
    "    segments_ids = [0]*len(tokens_a) + [1]*len(tokens_b)\n",
    "    \n",
    "    print(\"tokens: \\n{}\\n\".format(tag_bert_tokens))\n",
    "    print(\"segments_ids: \\n{}\\n\".format(segments_ids))\n",
    "\n",
    "    print([[i, j] for i, j in zip(range(len(tag_bert_tokens)), tag_bert_tokens)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "# ids: [101, 1996, 2158, 2253, 2000, 1996, 103, 1012, 102]\n\n# tokens: ['[CLS]', 'the', 'man', 'went', 'to', 'the', '[MASK]', '.', '[SEP]']\n\n# input_ids: tensor([[ 101, 1996, 2158, 2253, 2000, 1996,  103, 1012,  102]])\n\n# segments_tensors: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]])\n\n# true: \n['store']\n\n# scores: tensor([2.1023, 2.0991, 2.0584, 2.0334, 2.0319])\n\n# predicted_indexes: tensor([16511,  8303,  9549, 10844,  7001])\n\n# predicted_tokens: ['resorts', '##ress', 'convoy', 'cruiser', 'resort']\n\n\n"
    }
   ],
   "source": [
    "mask_id = [6]\n",
    "predict_mask(text1 + text2, tag_bert_tokens, mask_id, segments_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ● the man went to the store. masked=store\n",
    "### bert_based\n",
    "* predicted_tokens: ['door', 'window', 'bathroom', 'kitchen', 'doorway']\n",
    "\n",
    "### 1st version (small)\n",
    "* predicted_tokens: ['resorts', '##ress', 'convoy', 'cruiser', 'resort']\n",
    "\n",
    "### 1st version (large)\n",
    "* predicted_tokens: ['fabian', 'poems', 'breach', 'lenny', 'andreas']\n",
    "\n",
    "### 2nd version no mask（small）\n",
    "* predicted_tokens: ['formerly', 'https', '##rcus', 'previously', 'subsp']\n",
    "\n",
    "### 2nd version no large\n",
    "* predicted_tokens: ['##ntes', '##oire', '##hdi', 'apron', '##ending']\n",
    "\n",
    "## ● he bought a gallon of milk. masked=milk\n",
    "### bert_based\n",
    "* predicted_tokens: ['milk', 'water', 'coffee', 'beer', 'wine']\n",
    "\n",
    "### 1st version (small)\n",
    "* predicted_tokens: ['##heart', '##rran', '##drome', '##stream', '##llon']\n",
    "\n",
    "### 1st version (large)\n",
    "* predicted_tokens: ['dos', 'lana', 'bind', 'edit', '##aves']\n",
    "\n",
    "### 2nd version no mask（small）\n",
    "* predicted_tokens: ['##uba', 'https', 'www', '##ensis', 'http']\n",
    "\n",
    "### 2nd version no large\n",
    "* predicted_tokens: ['##bedo', '##onate', '##riety', '##bution', '##enia']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ●隣接文予測\n",
    "### next_sentence_label: \n",
    "* next sentence classification loss: torch.LongTensor of shape [batch_size] with indices selected in [0, 1]. \n",
    "* 0 => next sentence is the continuation, \n",
    "* 1 => next sentence is a random sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1st version to reproduce our results in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"model_dir\"] = \"/Users/ikuhiro/Desktop/BERT/model/General_TinyBERT_4L_312D/\"\n",
    "os.environ[\"pytorch_model\"] = \"/Users/ikuhiro/Desktop/BERT/model/General_TinyBERT_4L_312D/pytorch_model.bin\"\n",
    "os.environ[\"vocab_txt\"] = \"/Users/ikuhiro/Desktop/BERT/model/General_TinyBERT_4L_312D/vocab.txt\"\n",
    "os.environ[\"bert_config\"] = \"/Users/ikuhiro/Desktop/BERT/model/General_TinyBERT_4L_312D/bert_config.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"model_dir\"] = \"/Users/ikuhiro/Desktop/BERT/model/General_TinyBERT_6L_768D/\"\n",
    "os.environ[\"pytorch_model\"] = \"/Users/ikuhiro/Desktop/BERT/model/General_TinyBERT_6L_768D/pytorch_model.bin\"\n",
    "os.environ[\"vocab_txt\"] = \"/Users/ikuhiro/Desktop/BERT/model/General_TinyBERT_6L_768D/vocab.txt\"\n",
    "os.environ[\"bert_config\"] = \"/Users/ikuhiro/Desktop/BERT/model/General_TinyBERT_6L_768D/bert_config.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2nd version (2019/11/18) trained with more (book+wiki) and no [MASK] corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"model_dir\"] = \"/Users/ikuhiro/Desktop/BERT/model/2nd_General_TinyBERT_4L_312D/\"\n",
    "os.environ[\"pytorch_model\"] = \"/Users/ikuhiro/Desktop/BERT/model/2nd_General_TinyBERT_4L_312D/pytorch_model.bin\"\n",
    "os.environ[\"vocab_txt\"] = \"/Users/ikuhiro/Desktop/BERT/model/2nd_General_TinyBERT_4L_312D/vocab.txt\"\n",
    "os.environ[\"bert_config\"] = \"/Users/ikuhiro/Desktop/BERT/model/2nd_General_TinyBERT_4L_312D/bert_config.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"model_dir\"] = \"/Users/ikuhiro/Desktop/BERT/model/2nd_General_TinyBERT_6L_768D/\"\n",
    "os.environ[\"pytorch_model\"] = \"/Users/ikuhiro/Desktop/BERT/model/2nd_General_TinyBERT_6L_768D/pytorch_model.bin\"\n",
    "os.environ[\"vocab_txt\"] = \"/Users/ikuhiro/Desktop/BERT/model/2nd_General_TinyBERT_6L_768D/vocab.txt\"\n",
    "os.environ[\"bert_config\"] = \"/Users/ikuhiro/Desktop/BERT/model/2nd_General_TinyBERT_6L_768D/bert_config.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IsNextSentence\n",
    "text1 = \"\"\"\n",
    "the man went to the store .\n",
    "\"\"\"\n",
    "text2 = \"\"\"\n",
    "he bought a gallon of milk .\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NotNextSentence\n",
    "text1 = \"\"\"\n",
    "the man went to the store .\n",
    "\"\"\"\n",
    "text2 = \"\"\"\n",
    "penguins are flightless .\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## bert_based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask語予測と隣接文予測をするためのBERTモデル\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tinyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask語予測と隣接文予測をするためのBERTモデル\n",
    "model = BertForNextSentencePrediction.from_pretrained(os.environ[\"model_dir\"])\n",
    "tokenizer = BertTokenizer.from_pretrained(os.environ[\"model_dir\"], do_lower_case=True, do_basic_tokenize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "# ids: [101, 1996, 2158, 2253, 2000, 1996, 3573, 1012, 102, 2002, 4149, 1037, 25234, 1997, 6501, 1012, 102]\n\n# tokens: ['[CLS]', 'the', 'man', 'went', 'to', 'the', 'store', '.', '[SEP]', 'he', 'bought', 'a', 'gallon', 'of', 'milk', '.', '[SEP]']\n\n# segments_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n\nsecond-text is random next: ['True'] (next sentence: ['5.86740']', random sentence: ['-5.31490'])\n"
    }
   ],
   "source": [
    "tokens_a = tokenizer.tokenize(text1)\n",
    "tokens_b = tokenizer.tokenize(text2)\n",
    "tokens_a = ['[CLS]'] + tokens_a + ['[SEP]']\n",
    "tokens_b = tokens_b + ['[SEP]']\n",
    "tag_bert_tokens = tokens_a + tokens_b\n",
    "segments_ids = [0]*len(tokens_a) + [1]*len(tokens_b)\n",
    "\n",
    "ids = tokenizer.convert_tokens_to_ids(tag_bert_tokens)\n",
    "print(\"# ids: {}\\n\".format(ids))\n",
    "print(\"# tokens: {}\\n\".format(tokenizer.convert_ids_to_tokens(ids)))\n",
    "print(\"# segments_ids: {}\\n\".format(segments_ids))\n",
    "\n",
    "# ベクトルを作成する\n",
    "input_ids = torch.tensor([ids])  # unsqueeze(0) / reshape(1, -1)\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, token_type_ids=segments_tensors)\n",
    "    predictions = outputs[0]\n",
    "\n",
    "print(\"second-text is random next: ['{}'] (next sentence: ['{:.5f}']', random sentence: ['{:.5f}'])\".format(bool(1-np.argmax(predictions)), predictions[0], predictions[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-98-9105f495dcae>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-98-9105f495dcae>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    * second-text is random next: True (next sentence: 6.07, random sentence: -5.63)\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## ● the man went to the store . he bought a gallon of milk . -> IsNextSentence\n",
    "### bert_based\n",
    "* second-text is random next: ['True'] (next sentence: ['5.86740']', random sentence: ['-5.31490'])\n",
    "\n",
    "### 1st version (small)\n",
    "* second-text is random next: ['True'] (next sentence: ['0.02345']', random sentence: ['-0.02809'])\n",
    "\n",
    "### 1st version (large)\n",
    "* second-text is random next: ['True'] (next sentence: ['0.10266']', random sentence: ['-0.13284'])\n",
    "\n",
    "### 2nd version no mask（small）\n",
    "* second-text is random next: ['True'] (next sentence: ['-0.00289']', random sentence: ['-0.00343'])\n",
    "\n",
    "### 2nd version no large\n",
    "* second-text is random next: ['True'] (next sentence: ['0.00094'], random sentence: ['-0.00591'])\n",
    "\n",
    "## ● the man went to the store . penguins are flightless . -> NotNextSentence\n",
    "### bert_based\n",
    "* second-text is random next: ['False'] (next sentence: ['-2.93883']', random sentence: ['5.82727'])\n",
    "\n",
    "### 1st version (small)\n",
    "* second-text is random next: ['True'] (next sentence: ['0.01661']', random sentence: ['-0.00665'])\n",
    "\n",
    "### 1st version (large)\n",
    "* second-text is random next: ['True'] (next sentence: ['0.14387']', random sentence: ['0.02148'])\n",
    "\n",
    "### 2nd version no mask（small）\n",
    "* second-text is random next: ['True'] (next sentence: ['0.00018'], random sentence: ['-0.00087'])\n",
    "\n",
    "### 2nd version no large\n",
    "* second-text is random next: ['True'] (next sentence: ['0.00186'], random sentence: ['0.00106'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}