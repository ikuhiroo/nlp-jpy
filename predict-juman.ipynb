{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertForPreTraining, modeling\n",
    "from pyknp import Juman\n",
    "import os\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ●環境変数設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"pytorch_model\"] = \"./Japanese_L-12_H-768_A-12_E-30_BPE/pytorch_model.bin\"\n",
    "os.environ[\"vocab_txt\"] = \"./Japanese_L-12_H-768_A-12_E-30_BPE/vocab.txt\"\n",
    "os.environ[\"bert_config\"] = \"./Japanese_L-12_H-768_A-12_E-30_BPE/bert_config.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"attention_probs_dropout_prob\": 0.1,\n",
      "    \"hidden_act\": \"gelu\",\n",
      "    \"hidden_dropout_prob\": 0.1,\n",
      "    \"hidden_size\": 768,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 3072,\n",
      "    \"max_position_embeddings\": 512,\n",
      "    \"num_attention_heads\": 12,\n",
      "    \"num_hidden_layers\": 12,\n",
      "    \"type_vocab_size\": 2,\n",
      "    \"vocab_size\": 32006\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open(os.environ[\"bert_config\"], \"r\") as f:\n",
    "    jsonData = json.load(f)\n",
    "    print(json.dumps(jsonData, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ●Bert設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = modeling.BertConfig(\n",
    "    attention_probs_dropout_prob=jsonData[\"attention_probs_dropout_prob\"],\n",
    "    hidden_act=jsonData[\"hidden_act\"],\n",
    "    hidden_dropout_prob=jsonData[\"hidden_dropout_prob\"],\n",
    "    hidden_size=jsonData[\"hidden_size\"],\n",
    "    initializer_range=jsonData[\"initializer_range\"],\n",
    "    intermediate_size=jsonData[\"intermediate_size\"],\n",
    "    max_position_embeddings=jsonData[\"max_position_embeddings\"],\n",
    "    num_attention_heads=jsonData[\"num_attention_heads\"],\n",
    "    num_hidden_layers=jsonData[\"num_hidden_layers\"],\n",
    "    type_vocab_size=jsonData[\"type_vocab_size\"],\n",
    "    vocab_size_or_config_json_file=jsonData[\"vocab_size\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask語予測と隣接文予測をするためのBERTモデル\n",
    "model = BertForPreTraining(config=config)\n",
    "model.load_state_dict(torch.load(os.environ['pytorch_model']))\n",
    "\n",
    "# do_lower_case=False, do_basic_tokenize=False\n",
    "tokenizer = BertTokenizer(os.environ[\"vocab_txt\"], do_lower_case=False, do_basic_tokenize=False)\n",
    "\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ●juman++でトークン化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JumanTokenizer():\n",
    "    def __init__(self):\n",
    "        self.juman = Juman()\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        result = self.juman.analysis(text)\n",
    "        return [mrph.midasi for mrph in result.mrph_list()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ●マスク語予測\n",
    "### masked_lm_labels: \n",
    "* masked language modeling labels: torch.LongTensor of shape [batch_size, sequence_length] with indices selected in [-1, 0, ..., vocab_size]. \n",
    "* All labels set to -1 are ignored (masked), the loss is only computed for the labels set in [0, ..., vocab_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_mask(text, tag_bert_tokens, mask_id):\n",
    "    true_tokens = []\n",
    "    for i in mask_id:\n",
    "        true_tokens.append(tag_bert_tokens[i]) \n",
    "        tag_bert_tokens[i] = \"[MASK]\"\n",
    "    print(\"# tokens: \\n{}\\n\".format(tag_bert_tokens))\n",
    "\n",
    "    ids = tokenizer.convert_tokens_to_ids(tag_bert_tokens)\n",
    "\n",
    "    # ベクトルを作成する\n",
    "    tokens_tensor = torch.tensor(ids).reshape(1, -1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output, _ = model(tokens_tensor)\n",
    "\n",
    "    # 上位10トークンの確率\n",
    "    print(\"# true: \\n{}\\n\".format(true_tokens))\n",
    "    print(\"# predicted: \\n\")\n",
    "    for i in mask_id:\n",
    "        logits = output[0][i].sort()[0]\n",
    "        predicted_ids = output[0][i].sort()[1]\n",
    "\n",
    "        predicted_mask = [tokenizer.ids_to_tokens[i.item()] for i in output[0][i].argsort()[-15:]][::-1]\n",
    "\n",
    "        m = nn.Softmax(dim=-1)\n",
    "        pp = [\"{:.2f}%\".format(i*100) for i in m(logits).sort()[0][-15:]][::-1]\n",
    "\n",
    "        print([\"{}: {}\".format(i, j) for i, j in zip(predicted_mask, pp)])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"\"\"\n",
    "わたしはコンピュータ関係の仕事をしていますが、あなたは？\n",
    "\"\"\"\n",
    "text2 = \"\"\"\n",
    "わたしは居酒屋で働いています。\n",
    "\"\"\"\n",
    "\n",
    "# 文章数\n",
    "cnt = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analysis is done ignoring \"\\n\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: \n",
      "['[CLS]', 'わたし', 'は', 'コンピュータ', '関係', 'の', '仕事', 'を', 'して', 'い', 'ます', 'が', '、', 'あなた', 'は', '？', '[SEP]']\n",
      "\n",
      "[[0, '[CLS]'], [1, 'わたし'], [2, 'は'], [3, 'コンピュータ'], [4, '関係'], [5, 'の'], [6, '仕事'], [7, 'を'], [8, 'して'], [9, 'い'], [10, 'ます'], [11, 'が'], [12, '、'], [13, 'あなた'], [14, 'は'], [15, '？'], [16, '[SEP]']]\n"
     ]
    }
   ],
   "source": [
    "juman_tokenizer = JumanTokenizer()\n",
    "if cnt == 1:\n",
    "    text2 = \"\"\n",
    "    tokens = juman_tokenizer.tokenize(text1)\n",
    "    # print(\"juman++: {}\\n\".format(tokens))\n",
    "\n",
    "    bert_tokens = tokenizer.tokenize(\" \".join(tokens))\n",
    "    # print(\"BertTokenizer: {}\\n\".format(bert_tokens))\n",
    "\n",
    "    tag_bert_tokens = [\"[CLS]\"] + bert_tokens[:126] + [\"[SEP]\"]\n",
    "    print(\"tokens: \\n{}\\n\".format(tag_bert_tokens))\n",
    "\n",
    "    print([[i, j] for i, j in zip(range(len(tag_bert_tokens)), tag_bert_tokens)])\n",
    "\n",
    "elif cnt == 2:\n",
    "    tokens1 = juman_tokenizer.tokenize(text1)\n",
    "    tokens2 = juman_tokenizer.tokenize(text2)\n",
    "    # print(\"juman++: {} {}\".format(tokens1, tokens2))\n",
    "\n",
    "    bert_tokens1 = tokenizer.tokenize(\" \".join(tokens1))\n",
    "    bert_tokens2 = tokenizer.tokenize(\" \".join(tokens2))\n",
    "    # print(\"BertTokenizer: {} {}\".format(bert_tokens1, bert_tokens2))\n",
    "\n",
    "    tag_bert_tokens = (\n",
    "        [\"[CLS]\"] + bert_tokens1[:126] + [\"[SEP]\"] + bert_tokens2[:126] + [\"[SEP]\"]\n",
    "    )\n",
    "    print(\"tokens: \\n{}\\n\".format(tag_bert_tokens))\n",
    "\n",
    "    print([[i, j] for i, j in zip(range(len(tag_bert_tokens)), tag_bert_tokens)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "●juman++BPE\n",
      "# tokens: \n",
      "['[CLS]', 'わたし', 'は', 'コンピュータ', '関係', 'の', '仕事', 'を', 'して', 'い', 'ます', 'が', '、', '[MASK]', 'は', '？', '[SEP]']\n",
      "\n",
      "# true: \n",
      "['あなた']\n",
      "\n",
      "# predicted: \n",
      "\n",
      "['それ: 19.58%', '私: 11.35%', 'わたし: 8.60%', 'これ: 4.91%', '彼女: 3.32%', 'あなた: 3.21%', '仕事: 2.87%', '名前: 1.50%', '理由: 1.16%', '答え: 1.15%', '彼: 1.06%', '僕: 0.99%', '詳細: 0.85%', '中身: 0.79%', '結果: 0.76%']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mask_id = [13]\n",
    "print(\"●juman++BPE\")\n",
    "predict_mask(text1 + text2, tag_bert_tokens, mask_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ●隣接文予測\n",
    "### next_sentence_label: \n",
    "* next sentence classification loss: torch.LongTensor of shape [batch_size] with indices selected in [0, 1]. \n",
    "* 0 => next sentence is the continuation, \n",
    "* 1 => next sentence is a random sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"\"\"\n",
    "梅雨の話をしましょう？\n",
    "\"\"\"\n",
    "text2 = \"\"\"\n",
    "ワールドカップでベスト４に入ります。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analysis is done ignoring \"\\n\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text1: \n",
      "梅雨の話をしましょう？\n",
      "\n",
      "text2: \n",
      "ワールドカップでベスト４に入ります。\n",
      "\n",
      "tokens: \n",
      "['[CLS]', '梅雨', 'の', '話', 'を', 'し', 'ましょう', '？', '[SEP]', 'ワールドカップ', 'で', 'ベスト', '４', 'に', '入り', 'ます', '。', '[SEP]']\n",
      "\n",
      "next sentence: 84.28%\n",
      "random sentence: 15.72%\n",
      "\n",
      "second-text is random next: False\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analysis is done ignoring \"\\n\".\n"
     ]
    }
   ],
   "source": [
    "juman_tokenizer = JumanTokenizer()\n",
    "tokens1 = juman_tokenizer.tokenize(text1)\n",
    "tokens2 = juman_tokenizer.tokenize(text2)\n",
    "# print(\"juman++: {} {}\".format(tokens1, tokens2))\n",
    "\n",
    "bert_tokens1 = tokenizer.tokenize(\" \".join(tokens1))\n",
    "bert_tokens2 = tokenizer.tokenize(\" \".join(tokens2))\n",
    "# print(\"BertTokenizer: {} {}\".format(bert_tokens1, bert_tokens2))\n",
    "\n",
    "tag_bert_tokens = (\n",
    "    [\"[CLS]\"] + bert_tokens1[:126] + [\"[SEP]\"] + bert_tokens2[:126] + [\"[SEP]\"]\n",
    ")\n",
    "\n",
    "print(\"text1: {}\\ntext2: {}\".format(text1, text2))\n",
    "print(\"tokens: \\n{}\\n\".format(tag_bert_tokens))\n",
    "# print(\"len: {}\\n\".format(len(tag_bert_tokens)))\n",
    "\n",
    "ids = tokenizer.convert_tokens_to_ids(tag_bert_tokens)\n",
    "# print(\"idを振る: {}\\n\".format(ids))\n",
    "\n",
    "# ベクトルを作成する\n",
    "tokens_tensor = torch.tensor(ids).reshape(1, -1)\n",
    "# print(tokens_tensor)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output, label = model(tokens_tensor)\n",
    "    \n",
    "m = nn.Softmax(dim=-1)\n",
    "pp = [\"{:.2f}%\".format(i*100) for i in m(label[0])]\n",
    "\n",
    "print(\"next sentence: {}\\nrandom sentence: {}\\n\".format(pp[0], pp[1]))\n",
    "print(\"second-text is random next: {}\\n\".format(bool(np.argmax(pp))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
