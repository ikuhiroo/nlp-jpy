{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(\"modules\")\n",
    "from keras_bert import load_trained_model_from_checkpoint\n",
    "from keras.utils import plot_model\n",
    "\n",
    "import sentencepiece as spm\n",
    "import codecs\n",
    "import numpy as np\n",
    "import copy\n",
    "import logging\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 環境変数設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"config_path\"] = \"./keras_model/bert_config.json\"\n",
    "os.environ[\"checkpoint_path\"] = \"./keras_model/model.ckpt-1400000\"\n",
    "os.environ[\"dict_path\"] = \"./keras_model/wiki-ja.vocab\"\n",
    "os.environ[\"wiki_model\"] = \"./keras_model/wiki-ja.model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = os.environ[\"config_path\"]\n",
    "checkpoint_path = os.environ[\"checkpoint_path\"]\n",
    "dict_path = os.environ[\"dict_path\"]\n",
    "maxlen = 512\n",
    "bert_dim = 768"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modelのロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/ikuhiro/.pyenv/versions/3.6.3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/ikuhiro/.pyenv/versions/3.6.3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3363: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model = load_trained_model_from_checkpoint(config_path, checkpoint_path, training=True)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SentencePieceProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(os.environ[\"wiki_model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_dict = {}\n",
    "with codecs.open(dict_path, \"r\", \"utf8\") as reader:\n",
    "    for line in reader:\n",
    "        token = line.split()[0]\n",
    "        token_dict[token] = len(token_dict)\n",
    "token_dict_rev = {v: k for k, v in token_dict.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### マスク単語予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_masks(tokens, masks_id=None):\n",
    "    # マスク位置をランダムにするかどうか\n",
    "    if masks_id is None:\n",
    "        mask_rate=0.15\n",
    "        masks_id = (\n",
    "            sorted(\n",
    "                random.sample(range(2, len(tokens)), k=int(len(tokens) * mask_rate))\n",
    "            ),\n",
    "        )\n",
    "    true_tokens = copy.copy(tokens)\n",
    "\n",
    "    ## tokensをdictにする\n",
    "    print(\"# tokens: \\n{}\\n\".format(tokens))\n",
    "    tokenized_dict = {}\n",
    "    for i in range(len(tokens)):\n",
    "        tokenized_dict[i] = tokens[i]\n",
    "\n",
    "    label = []\n",
    "    masks_id = masks_id\n",
    "\n",
    "    for i in masks_id:\n",
    "        label.append(tokens[i])\n",
    "        tokens[i] = \"[MASK]\"\n",
    "    print(\"# tokens:\\n{}\\n\".format(tokens))\n",
    "    print(\"# true:{}\".format(label))\n",
    "    \n",
    "    indices = np.zeros((1, maxlen), dtype=np.float32)\n",
    "    segments = np.zeros((1, maxlen), dtype=np.float32)\n",
    "    masks = np.asarray([[0] * 512])\n",
    "    for i in masks_id:\n",
    "        masks[0][i] = 1\n",
    "\n",
    "    ## ベクトル化する\n",
    "    for t, token in enumerate(tokens):\n",
    "        try:\n",
    "            indices[0, t] = sp.piece_to_id(token)\n",
    "        except:\n",
    "            logging.warn(f\"{token} is unknown.\")\n",
    "            indices[0, t] = sp.piece_to_id(\"<unk>\")\n",
    "    vector = model.predict([indices, segments, masks])[0]\n",
    "\n",
    "    ### 32000tokenから最大のidを返す\n",
    "    predicts = np.argmax(vector, axis=-1)\n",
    "    predicts_sort = np.sort(vector, axis=-1)\n",
    "    predicts_args = np.argsort(vector, axis=-1)\n",
    "\n",
    "    result_predictd = {}\n",
    "    ranknum = 10\n",
    "    for i in range(len(masks_id)):\n",
    "        str = \"# true: {}\\n\".format(label[i])\n",
    "        for j in range(1, ranknum + 1):\n",
    "            # i番目に大きいindexを用いる\n",
    "            predicts[0][masks_id[i]] = predicts_args[0][masks_id[i]][-j]\n",
    "            # 予測単語と確率\n",
    "            str += \"{}: {} ({:.2f}) \".format(\n",
    "                j,\n",
    "                list(\n",
    "                    map(\n",
    "                        lambda x: token_dict_rev[x],\n",
    "                        predicts[0][masks_id[i] : masks_id[i] + 1],\n",
    "                    )\n",
    "                )[0],\n",
    "                (predicts_sort[0][masks_id[i]][-j]) * 100,\n",
    "            )\n",
    "        str += \"\\n\"\n",
    "        print(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence_1 = \"\"\"\n",
    "わたしはコンピュータ関係の仕事をしていますが、あなたは？\n",
    "\"\"\"\n",
    "test_sentence_2 = \"\"\"\n",
    "わたしは居酒屋で働いています。\n",
    "\"\"\"\n",
    "\n",
    "# 文章数\n",
    "c_num = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: \n",
      "\n",
      "わたしはコンピュータ関係の仕事をしていますが、あなたは？\n",
      "\n",
      "\n",
      "tokens: \n",
      "[(0, '[CLS]'), (1, '▁'), (2, 'わたし'), (3, 'は'), (4, 'コンピュータ'), (5, '関係の'), (6, '仕事を'), (7, 'し'), (8, 'ています'), (9, 'が'), (10, '、'), (11, 'あなた'), (12, 'は'), (13, '?'), (14, '[SEP]')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if c_num == 1:\n",
    "    print(\"text: \\n{}\\n\".format(test_sentence_1))\n",
    "\n",
    "    ## sentencepieceでtokenize\n",
    "    tokens = []\n",
    "    tokens.append(\"[CLS]\")\n",
    "    tokens.extend(sp.encode_as_pieces(test_sentence_1))\n",
    "    tokens.append(\"[SEP]\")\n",
    "    print(\"tokens: \\n{}\\n\".format([(i, j) for i, j in zip(range(len(tokens)), tokens)]))\n",
    "\n",
    "elif c_num == 2:\n",
    "    print(\"text: \\n{}{}\\n\".format(test_sentence_1, test_sentence_2))\n",
    "\n",
    "    ## sentencepieceでtokenize\n",
    "    tokens = []\n",
    "    tokens.append(\"[CLS]\")\n",
    "    tokens.extend(sp.encode_as_pieces(test_sentence_1))\n",
    "    tokens.append(\"[SEP]\")\n",
    "    tokens.extend(sp.encode_as_pieces(test_sentence_2))\n",
    "    tokens.append(\"[SEP]\")\n",
    "    print(\"tokens: \\n{}\\n\".format(tokens))\n",
    "    print(\"tokens: \\n{}\\n\".format([(i, j) for i, j in zip(range(len(tokens)), tokens)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "●SentencePiece\n",
      "# tokens: \n",
      "['[CLS]', '▁', 'わたし', 'は', 'コンピュータ', '関係の', '仕事を', 'し', 'ています', 'が', '、', 'あなた', 'は', '?', '[SEP]']\n",
      "\n",
      "# tokens:\n",
      "['[CLS]', '▁', 'わたし', 'は', 'コンピュータ', '関係の', '仕事を', 'し', 'ています', 'が', '、', '[MASK]', 'は', '?', '[SEP]']\n",
      "\n",
      "# true:['あなた']\n",
      "# true: あなた\n",
      "1: 今 (9.46) 2: これから (8.42) 3: 次 (3.45) 4: どう (2.82) 5: 具体的に (2.19) 6: いつか (2.01) 7: 結局 (1.61) 8: そこに (1.60) 9: そのために (1.49) 10: 意味 (1.47) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"●SentencePiece\")\n",
    "predict_masks(tokens, masks_id=[11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ●隣接文予測: check whether the two sentences are continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Next_Sentence_Prediction(test_sentence_1, test_sentence_2):\n",
    "    print(\"### first-text:\\n{}\\n\".format(test_sentence_1))\n",
    "    print(\"### second-text:\\n{}\\n\".format(test_sentence_2))\n",
    "\n",
    "    ## truncate\n",
    "    tokens1 = []\n",
    "    tokens1.append(\"[CLS]\")\n",
    "    tokens1.extend(sp.encode_as_pieces(test_sentence_1))\n",
    "    tokens1.append(\"[SEP]\")\n",
    "\n",
    "    tokens2 = []\n",
    "    tokens2.append(\"[CLS]\")\n",
    "    tokens2.extend(sp.encode_as_pieces(test_sentence_2))\n",
    "    tokens2.append(\"[SEP]\")\n",
    "\n",
    "    ## pack\n",
    "    first_packed_tokens = tokens1\n",
    "    second_packed_tokens = tokens2[1:]\n",
    "    tokens = tokens1 + tokens2[1:]\n",
    "\n",
    "    print(\"### tokens:\\n{}\\n\".format(tokens))\n",
    "\n",
    "    first_len = len(first_packed_tokens)\n",
    "    second_len = len(second_packed_tokens)\n",
    "    pad_len = 512 - first_len - second_len\n",
    "\n",
    "    # sentenseの区別\n",
    "    segments = np.zeros((1, maxlen), dtype=np.float32)\n",
    "    segments[0][first_len : first_len + second_len] = [1] * (second_len)\n",
    "\n",
    "    ## ids\n",
    "    indices = np.zeros((1, maxlen), dtype=np.float32)\n",
    "    for t, token in enumerate(tokens):\n",
    "        try:\n",
    "            indices[0, t] = sp.piece_to_id(token)\n",
    "        except:\n",
    "            print(f\"{token} is unknown.\")\n",
    "            indices[0, t] = sp.piece_to_id(\"<unk>\")\n",
    "\n",
    "    ## mask位置\n",
    "    masks = np.array([[0] * 512])\n",
    "\n",
    "    ## predicts[1]\n",
    "    print(\"### 結果:\")\n",
    "    predicts = model.predict([indices, segments, masks])[1][0]\n",
    "    print(\"next sentence: {}\\nrandom sentence: {}\\n\".format(predicts[0], predicts[1]))\n",
    "    print(\"●second-text is random next: {}\\n\".format(bool(np.argmax(predicts))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence_1 = \"\"\"\n",
    "梅雨の話をしましょう？\n",
    "\"\"\"\n",
    "test_sentence_2 = \"\"\"\n",
    "ワールドカップでベスト４に入ります。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------\n",
      "### memo\n",
      "\n",
      "\n",
      "### first-text:\n",
      "\n",
      "梅雨の話をしましょう？\n",
      "\n",
      "\n",
      "### second-text:\n",
      "\n",
      "ワールドカップでベスト４に入ります。\n",
      "\n",
      "\n",
      "### tokens:\n",
      "['[CLS]', '▁', '梅', '雨', 'の', '話を', 'し', 'ましょう', '?', '[SEP]', '▁', 'ワールドカップ', 'で', 'ベスト', '4', 'に入り', 'ます', '。', '[SEP]']\n",
      "\n",
      "### 結果:\n",
      "next sentence: 0.011785382404923439\n",
      "random sentence: 0.9882146120071411\n",
      "\n",
      "●second-text is random next: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"-------------------------------------------------------------------------------------\"\n",
    ")\n",
    "print(\"### memo\\n\\n\")\n",
    "\n",
    "Next_Sentence_Prediction(test_sentence_1, test_sentence_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
