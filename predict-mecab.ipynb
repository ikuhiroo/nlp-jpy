{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertForPreTraining, modeling\n",
    "from pyknp import Juman\n",
    "import os\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ●環境変数設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"pytorch_model\"] = \"./mecab-PyTorch/pytorch_model.bin\"\n",
    "os.environ[\"vocab_txt\"] = \"./mecab-PyTorch/vocab.txt\"\n",
    "os.environ[\"bert_config\"] = \"./mecab-PyTorch/bert_config.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"attention_probs_dropout_prob\": 0.1,\n",
      "    \"hidden_act\": \"gelu\",\n",
      "    \"hidden_dropout_prob\": 0.1,\n",
      "    \"hidden_size\": 768,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 3072,\n",
      "    \"max_position_embeddings\": 512,\n",
      "    \"num_attention_heads\": 12,\n",
      "    \"num_hidden_layers\": 12,\n",
      "    \"type_vocab_size\": 2,\n",
      "    \"vocab_size\": 32005\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open(os.environ[\"bert_config\"], \"r\") as f:\n",
    "    jsonData = json.load(f)\n",
    "    print(json.dumps(jsonData, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ●Bert設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = modeling.BertConfig(\n",
    "    attention_probs_dropout_prob=jsonData[\"attention_probs_dropout_prob\"],\n",
    "    hidden_act=jsonData[\"hidden_act\"],\n",
    "    hidden_dropout_prob=jsonData[\"hidden_dropout_prob\"],\n",
    "    hidden_size=jsonData[\"hidden_size\"],\n",
    "    initializer_range=jsonData[\"initializer_range\"],\n",
    "    intermediate_size=jsonData[\"intermediate_size\"],\n",
    "    max_position_embeddings=jsonData[\"max_position_embeddings\"],\n",
    "    num_attention_heads=jsonData[\"num_attention_heads\"],\n",
    "    num_hidden_layers=jsonData[\"num_hidden_layers\"],\n",
    "    type_vocab_size=jsonData[\"type_vocab_size\"],\n",
    "    vocab_size_or_config_json_file=jsonData[\"vocab_size\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask語予測と隣接文予測をするためのBERTモデル\n",
    "model = BertForPreTraining(config=config)\n",
    "model.load_state_dict(torch.load(os.environ['pytorch_model']))\n",
    "\n",
    "# do_lower_case=False, do_basic_tokenize=False\n",
    "tokenizer = BertTokenizer(os.environ[\"vocab_txt\"], do_lower_case=False, do_basic_tokenize=False)\n",
    "\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ●MeCab + NEologでトークン化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "class MecabTokenizer():\n",
    "    def __init__(self):\n",
    "        self.mecab = MeCab.Tagger(\"-Ochasen -d /usr/local/lib/mecab/dic/mecab-ipadic-neologd\")\n",
    "        self.mecab.parse(\"\")\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        node = self.mecab.parseToNode(text)\n",
    "        result = []\n",
    "        while node:\n",
    "            result.append(node.surface)\n",
    "            node = node.next\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ●マスク語予測\n",
    "### masked_lm_labels: \n",
    "* masked language modeling labels: torch.LongTensor of shape [batch_size, sequence_length] with indices selected in [-1, 0, ..., vocab_size]. \n",
    "* All labels set to -1 are ignored (masked), the loss is only computed for the labels set in [0, ..., vocab_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_mask(text, tag_bert_tokens, mask_id):\n",
    "    true_tokens = []\n",
    "    for i in mask_id:\n",
    "        true_tokens.append(tag_bert_tokens[i]) \n",
    "        tag_bert_tokens[i] = \"[MASK]\"\n",
    "    print(\"# tokens: \\n{}\\n\".format(tag_bert_tokens))\n",
    "\n",
    "    ids = tokenizer.convert_tokens_to_ids(tag_bert_tokens)\n",
    "\n",
    "    # ベクトルを作成する\n",
    "    tokens_tensor = torch.tensor(ids).reshape(1, -1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output, _ = model(tokens_tensor)\n",
    "\n",
    "    # 上位10トークンの確率\n",
    "    print(\"# true: \\n{}\\n\".format(true_tokens))\n",
    "    print(\"# predicted: \\n\")\n",
    "    for i in mask_id:\n",
    "        logits = output[0][i].sort()[0]\n",
    "        predicted_ids = output[0][i].sort()[1]\n",
    "\n",
    "        predicted_mask = [tokenizer.ids_to_tokens[i.item()] for i in output[0][i].argsort()[-15:]][::-1]\n",
    "\n",
    "        m = nn.Softmax(dim=-1)\n",
    "        pp = [\"{:.2f}%\".format(i*100) for i in m(logits).sort()[0][-15:]][::-1]\n",
    "\n",
    "        print([\"{}: {}\".format(i, j) for i, j in zip(predicted_mask, pp)])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"\"\"\n",
    "包丁とは、調理に使う刃物のことであり、AI、ロボット、焼き鳥など多くの種類がある。\n",
    "\"\"\"\n",
    "text2 = \"\"\"\n",
    "あなたは？\n",
    "\"\"\"\n",
    "\n",
    "# 文書数\n",
    "cnt = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: \n",
      "['[CLS]', '包丁', 'と', 'は', '、', '調理', 'に', '使う', '刃物', 'の', 'こと', 'で', 'あり', '、', 'AI', '、', 'ロボット', '、', '焼き鳥', 'など', '多く', 'の', '種類', 'が', 'ある', '。', '[SEP]']\n",
      "\n",
      "[[0, '[CLS]'], [1, '包丁'], [2, 'と'], [3, 'は'], [4, '、'], [5, '調理'], [6, 'に'], [7, '使う'], [8, '刃物'], [9, 'の'], [10, 'こと'], [11, 'で'], [12, 'あり'], [13, '、'], [14, 'AI'], [15, '、'], [16, 'ロボット'], [17, '、'], [18, '焼き鳥'], [19, 'など'], [20, '多く'], [21, 'の'], [22, '種類'], [23, 'が'], [24, 'ある'], [25, '。'], [26, '[SEP]']]\n"
     ]
    }
   ],
   "source": [
    "Mecab_Tokenizer = MecabTokenizer()\n",
    "if cnt == 1:\n",
    "    text2 = \"\"\n",
    "    tokens = Mecab_Tokenizer.tokenize(text1)\n",
    "    # print(\"juman++: {}\\n\".format(tokens))\n",
    "\n",
    "    bert_tokens = tokenizer.tokenize(\" \".join(tokens))\n",
    "    # print(\"BertTokenizer: {}\\n\".format(bert_tokens))\n",
    "\n",
    "    tag_bert_tokens = [\"[CLS]\"] + bert_tokens[:126] + [\"[SEP]\"]\n",
    "    print(\"tokens: \\n{}\\n\".format(tag_bert_tokens))\n",
    "\n",
    "    print([[i, j] for i, j in zip(range(len(tag_bert_tokens)), tag_bert_tokens)])\n",
    "\n",
    "elif cnt == 2:\n",
    "    tokens_a = Mecab_Tokenizer.tokenize(text1)\n",
    "    tokens_b = Mecab_Tokenizer.tokenize(text2)\n",
    "    tokens_a = tokenizer.tokenize(\" \".join(tokens_a))\n",
    "    tokens_b = tokenizer.tokenize(\" \".join(tokens_b))\n",
    "    tokens_a = ['[CLS]'] + tokens_a + ['[SEP]']\n",
    "    tokens_b = tokens_b + ['[SEP]']\n",
    "    tag_bert_tokens = tokens_a + tokens_b\n",
    "    \n",
    "    print(\"tokens: \\n{}\\n\".format(tag_bert_tokens))\n",
    "\n",
    "    print([[i, j] for i, j in zip(range(len(tag_bert_tokens)), tag_bert_tokens)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "●mecab\n",
      "# tokens: \n",
      "['[CLS]', '包丁', 'と', 'は', '、', '調理', 'に', '使う', '刃物', 'の', 'こと', 'で', 'あり', '、', 'AI', '、', '[MASK]', '、', '焼き鳥', 'など', '多く', 'の', '種類', 'が', 'ある', '。', '[SEP]']\n",
      "\n",
      "# true: \n",
      "['ロボット']\n",
      "\n",
      "# predicted: \n",
      "\n",
      "['[UNK]: 47.00%', '包丁: 12.39%', '焼き鳥: 2.20%', '鍋: 1.37%', '刃物: 1.09%', '箸: 0.94%', 'フライパン: 0.93%', 'ナイフ: 0.90%', '餃子: 0.88%', 'カッター: 0.85%', '豆腐: 0.80%', '寿司: 0.69%', 'きゅうり: 0.67%', '天ぷら: 0.63%', '揚げ物: 0.55%']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mask_id = [16]\n",
    "print(\"●mecab\")\n",
    "predict_mask(text1 + text2, tag_bert_tokens, mask_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ●隣接文予測\n",
    "### next_sentence_label: \n",
    "* next sentence classification loss: torch.LongTensor of shape [batch_size] with indices selected in [0, 1]. \n",
    "* 0 => next sentence is the continuation, \n",
    "* 1 => next sentence is a random sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"\"\"\n",
    "包丁とは、調理に使う刃物のことである。\n",
    "\"\"\"\n",
    "text2 = \"\"\"\n",
    "出刃包丁、刺身包丁、菜切り包丁など、多くの種類がある。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text1: \n",
      "包丁とは、調理に使う刃物のことである。\n",
      "\n",
      "text2: \n",
      "出刃包丁、刺身包丁、菜切り包丁など、多くの種類がある。\n",
      "\n",
      "tokens: \n",
      "['[CLS]', '包丁', 'と', 'は', '、', '調理', 'に', '使う', '刃物', 'の', 'こと', 'で', 'ある', '。', '[SEP]', '[UNK]', '、', '[UNK]', '、', '[UNK]', 'など', '、', '多く', 'の', '種類', 'が', 'ある', '。', '[SEP]']\n",
      "\n",
      "next sentence: 20.38%\n",
      "random sentence: 79.62%\n",
      "\n",
      "second-text is random next: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Mecab_Tokenizer = MecabTokenizer()\n",
    "tokens1 = Mecab_Tokenizer.tokenize(text1)\n",
    "tokens2 = Mecab_Tokenizer.tokenize(text2)\n",
    "# print(\"juman++: {} {}\".format(tokens1, tokens2))\n",
    "\n",
    "bert_tokens1 = tokenizer.tokenize(\" \".join(tokens1))\n",
    "bert_tokens2 = tokenizer.tokenize(\" \".join(tokens2))\n",
    "# print(\"BertTokenizer: {} {}\".format(bert_tokens1, bert_tokens2))\n",
    "\n",
    "tag_bert_tokens = (\n",
    "    [\"[CLS]\"] + bert_tokens1[:126] + [\"[SEP]\"] + bert_tokens2[:126] + [\"[SEP]\"]\n",
    ")\n",
    "\n",
    "print(\"text1: {}\\ntext2: {}\".format(text1, text2))\n",
    "print(\"tokens: \\n{}\\n\".format(tag_bert_tokens))\n",
    "# print(\"len: {}\\n\".format(len(tag_bert_tokens)))\n",
    "\n",
    "ids = tokenizer.convert_tokens_to_ids(tag_bert_tokens)\n",
    "# print(\"idを振る: {}\\n\".format(ids))\n",
    "\n",
    "# ベクトルを作成する\n",
    "tokens_tensor = torch.tensor(ids).reshape(1, -1)\n",
    "# print(tokens_tensor)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output, label = model(tokens_tensor)\n",
    "    \n",
    "m = nn.Softmax(dim=-1)\n",
    "pp = [\"{:.2f}%\".format(i*100) for i in m(label[0])]\n",
    "\n",
    "print(\"next sentence: {}\\nrandom sentence: {}\\n\".format(pp[0], pp[1]))\n",
    "print(\"second-text is random next: {}\\n\".format(bool(np.argmax(pp))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
